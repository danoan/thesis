\chapter*{Introduction}\label{chapter:introduction}
\addcontentsline{toc}{chapter}{Introduction}


Is the universe continuous or discrete? The analyst would certainly say that it is continuous and it will point out the natural phenomena modeled by calculus and the successful applications in engineering. The computer scientist would surely not accept this idea because there is not such a thing as an unlimited space or unlimited time. The statistician would rather say that both are correct with a probability of $50\%$.

This thesis has no pretension to answer this intricate question and its philosophical consequences. Our contributions are much humbler than that. Applied mathematics have to face this duality every time an analytic solution is not available and this is not different with image processing, except that in the latter we have to deal with a third ingredient: \emph{digital objects}.

An image is a $2$D discrete representation of a projection of a much more complex $3$D world. The unit of an image is the pixel, and pixels lie in the digital grid, which is a regular sampling of the plane. Differently from most discretization schemes in numerical analysis, whose discretization points are allowed to be located anywhere, points in an image are constrained to a subset of $\mathbb{Z}^2$. This restriction lead to a serious problem if our image processing model is based on \emph{geometric measurements} of objects in the scene. How to compute geometric measurements of objects described by points in $\mathbb{Z}^2$?

A key argument of this thesis is that general discretizations of geometric measurements, e.g. perimeter, tangent, curvature, do not extend well to the digital world. A linear discretization of curvature proven convergent (in some sense) to the continuous definition do not say too much when this measurement is done in digital objects. First because we are not allowed to position the discretization points anywhere, and second because the convergence theorems usually tells us about convergence when the number of discretization points goes to infinity, which is very frustrating when coping with finite data. More appropriate would be a convergence theorem that relates the convergence speed with the resolution of the digital grid. This concept exists and it is called the \revision{\emph{multigrid convergence}}.

Recently, several digital estimators of tangent and curvature were proven multigrid convergent, but there is a lack of image processing models using such estimators. This thesis investigates the use of multigrid convergent estimators in models of image processing relying on a combinatorial optimization framework. The developed work is mainly concerned with image segmentation models using a digital version of the \emph{elastica energy}. The thesis is grouped in two parts:

\begin{itemize}
	\item[]{\textbf{Image Processing and Digital Geometry.} We review classical models of image processing (continuous and discrete), and we dedicate particular attention to those employing geometric properties as regularizers, mainly the curvature. At the end of this part, we briefly introduce basic concepts from digital geometry and the multigrid convergence definition, as well as some examples of multigrid convergent estimators.}
	\item[]{\textbf{Contributions.} In this part we grouped all our contributions. It is composed of four combinatorial models aiming elastica energy minimization. Some of them can be applied in image segmentation. The last model is the fastest one and is based on graph cuts.}
\end{itemize}

\subsubsection{Chapters outline}

\textbf{Part I: Image Processing and Digital Geometry}
\begin{itemize}
\item[]{\textbf{1. Variational models in Image Processing.} As many applied fields, image processing drinks from the source of continuum models, in particular those emerging from partial differential equations and signal processing.  The image is then modeled as an infinitely smooth function and, based on the principle of \emph{least energy}, energies (functionals) are proposed such that the image of minimum (or maximum) value gives us the answer of the problem. That is the so called \emph{variational approach}, that eventually is solved by the computation of its \emph{Euler-Lagrange equation} and it boils down to find the steady-state of a diffusion process. The classical \emph{Tikhonov} and \emph{total variation} for image denoising and inpainting follow this principle. Similarly, we have curve evolution approaches to segment  objects of interest in the scene.}
\item[]{\textbf{2. Discrete methods in Image Processing.} The continuous point of view is popular because it is, indeed, very powerful and produce satisfactorily results for several imaging problems as discussed in~\cref{chapter:variational-methods-in-image-processing}. An important drawback, however, is that it is still very difficult for a continuous model to preserve the discontinuities of an image along the edges of the objects present on it, which is the most important feature of an image (the human perception targets discontinuities in images). That is one of the reasons why discrete (combinatorial) approaches appear as an alternative. In this chapter, we review some combinatorial models inspired by \emph{Markov random fields} and we will be interested in a special class of energies emerging from its Gibbs energy: the \emph{submodular pseudo-boolean functions} class. In particular, we are going to point out its relationship with \emph{graph cuts} and present some graph cut based models applied in imaging at the end of the chapter.}
\item[]{\textbf{3. Curvature as regularizer.} We focus on models that employ curvature as a regularization term, both in continuous and discrete settings. In a second moment, we turn to the elastica energy and examine imaging models based on it for segmentation and inpainting. Finally, we describe combinatorial models that attempts to minimize the squared curvature.}
\item[]{\textbf{4. Digital Geometry.} It is often the case that we do not know a priori the mathematical expressions modeling the objects in an image. Sometimes, it is not even possible to get such expressions, or at least, it is very complicated. Therefore, we have to recognize shapes from their digital representation, possibly by recognizing primitives as lines, circles, etc. However, we should remember that the sampling on a regular grid  imposes special conditions on how the computation of geometric properties is done on digital objects. Digital geometry offers the proper tools to do such measurements and evaluate their convergence towards the measurements on the continuous representation of the object. In this chapter, we give a brief introduction to digital geometry and we define the multigrid convergence property for digital estimators of geometric properties.}
\end{itemize}

\textbf{Part II: Contributions}
\begin{itemize}
\item[]{\textbf{5. A combinatorial model for digital elastica shape optimization.} We propose a local combinatorial model to minimize the elastica energy using multigrid convergent estimators of length and curvature. We validate the model through experiments and we observe convergence to the shape of minimum elastica value in the \emph{free elastica problem}. At the end of the chapter, we sketch some \revision{attempts of} global optimization models.}
\item[]{\textbf{6. A 2-step evolution model driven by digital elastica minimization.} We propose to iteratively minimize a quadratic non-submodular pseudo boolean function to evolve an initial shape to another with lower elastica energy. The model is specially conceived for the \emph{curvature integral invariant estimator} described in~\cref{chapter:digital-geometry}. We present an application to image segmentation at the end of the chapter. }
\item[]{\textbf{7. A single step evolution model driven by digital elastica minimization.} It can be seen as an improved version of the previous model, though with some differences. In particular, the model is singled step and has an easier implementation. In this chapter we introduce the \emph{balance coefficient} and we set up the terrain for a graph cut based model.}
\item[]{\textbf{8. Digital elastica minimization via graph cuts.} The balance coefficient defined in the previous chapter is used to set up a cost function on the edges of \emph{candidate graphs}. The candidate graphs are derived from a neighborhood of shapes, and the solution, at each iteration, is chosen as the source component of a minimum cut in the candidate graphs with lowest digital elastica value. We observe convergence to the global optimum shape in the free elastica problem and we show how to use it in image segmentation. }
\item[]{\textbf{9. Result analysis.} We make a summary of the models developed in this thesis and we point out its pros and cons. At the end of the chapter, we present a comparison of them with a competitor model that uses curvature regularization in image segmentation. }
\end{itemize}

%\subsubsection{The continuum view}
%
%As many other applied fields, image processing drinks from the source of continuum models, in particular those emerging from partial differential equations and signal processing.  The image is then modeled as an infinitely smooth well behaved function and, based on the principle of \emph{least energy}, energies (functionals) are proposed such that the image of minimum (or maximum) value gives us the answer of the problem. That is the so called \emph{variational approach}, that eventually is solved by the computation of its \emph{Euler-Lagrange equation} and it boils down to find the steady-state of a diffusion process. The classical \emph{Tikhonov} and \emph{Total Variation} for image denoising and inpainting follow this principle. Similarly, we have curve evolution approaches to segment  objects of interest in the scene.
%
%From the theoretical point of view, images are not like ordinary functions, infinitely smooth. Imagine the picture of a table with several colored pencils over it. There is an infinite amount of discontinuity points along the edges of the pencils. The analysts are diligent fellows and they come up with a formal definition of a functional space suitable to shelter image-like functions: the \emph{bounded variation space}. If the energy is convex, we do not need to appeal to the energy continuity. There exist well established methods in convex optimization that handles non-smooth convex functions much better than the classical approach of assuming smoothness and then apply calculus. The total variation model is an example of this.
%
%The continuum point of view is popular because it is, indeed, very powerful and produce satisfactorily results for several imaging problems as discussed in~\cref{chapter:variational-methods-in-image-processing}. An important drawback, however, it is that still very difficult for a continuous model to preserve the discontinuity character of an image along the edges of its objects, the most important feature of the image. That is one of the reasons why discrete (combinatorial) approaches appear as an alternative.
%
%One may argue that the large majority of the continuous models are not analytically solvable and that numerical implementations, with a discretization scheme, are inevitably necessary. In fact, the choice of a discretization scheme is a big problem as well in continuous models, in particular in those whose energy has derivatives of elevated order. The discretization errors are added to machine errors and we can easily loose track of  what is really being computed. 
%
%
%\subsubsection{The discrete (combinatorial) view}
%
%When we refer to discrete models we are referring to a model in which there is a finite number of solution candidates. For example, a linear programming model is discrete in the sense that the solutions are limited by the number of vertices of the polytope defined by its constraints; an optimization problem in a graph, as finding its minimum cut, is also a discrete problem because there is a finite number of possible cuts.
%
%
%
%
%
%
%
%
%Our main object of study is the function $\mathcal{I}:\Omega \subset \mathbb{Z}^j \rightarrow [0,1]^k$. We have a grayscale digital image for $(j=2,k=1)$ and a three dimensional colored object for $(j=3,k=3)$ if we assume the RGB color scheme. Such objects are created by sampling a continuous domain and its quality is directly related with its resolution, or the number of samples one uses to discretize the domain. 
%
%Digital image processing has been developed since the 60s, but still an active field of research. Among the reasons, we can cite the advances in acquisition, which exponentially increases the amount of data to be treated; and the popularization of digital cameras. In fact, applications involving digital images are numerous. An autonomous vehicle must partition the frames coming from its camera in meaningful regions in order to identify roads, traffic signs, people, landscape and other vehicles (segmentation); satellite images are usually degraded due to limitations in transmission and storage devices and should be processed to enhance quality (denoising); in cinema, post-production editors might be faced with undesired objects in the scenes as cables or cameras that should be removed smoothly (inpainting). 
%
%The classical approach consists to solve an optimization problem. It is a reasonable strategy, as we are interested to find the best segmentation or the best image inpainting, the definition of best being application-based. Using information or assumptions intrinsic to the problem, we can make use of prior terms to guide the optimization process, as for example, the geometry of objects we wish to segment. In chapter \ref{chapter:regularization} we describe some classical models in image segmentation and the role of geometric priors.
%
%The curvature is an example of geometric prior with properties that makes it suitable for the segmentation of long and thin structures as blood vessels. Its use and the difficulties that come along with it are discussed in chapter \ref{chapter:curvature-prior}, which sets the ground for our digital approach. In chapter \ref{chapter:digital-geometry} we introduce some concepts of digital geometry, among them the multigrid convergence of a digital estimator. The goal of this chapter is to argue that multigrid convergent estimators of curvature should be preferred when computing curvature in digital data. 
%
%Finally, we describe the main product of this work in chapter \ref{chapter:digital-flow} and illustrate its results with several experiments and comparisons. In the appendix, the reader can found three other models developed during this thesis and considered relevant by the author for future investigation in the subject.
%
%\section*{Curvature and Elastica energy}
%\section*{Curvature as a prior}
%\section*{Continuous x Discrete x Digital}
%
%\section*{Contribution}
%\sketch{
%\begin{itemize}
%\item{Thesis structure}
%\item{Link between models}
%\item{Pros,cons each model}
%\end{itemize}
%}
