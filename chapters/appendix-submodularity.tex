\chapter{Submodularity}\label{chapter:submodularity}

The contents of this chapter are strongly based on the manuscript \cite{} and is included here for completeness.

Let $\mathbb{B} = \{0,1\}$. The function $f:\mathbb{B}^n\rightarrow\mathbb{R}$ is called a pseudo-boolean function.

\textbf{Set function equivalence:} A set function maps subsets to values. Consider any enumerable set which elements are indexed by $\mathbf{V}$. The function $g:2^V\rightarrow \mathcal{R}$ is a set function to the reals. The pseudo-boolean function $f$ can be interpreted as a set function by defining $V=\{1,2,\cdots,n\}$. Each subset of $V$ is mapped to a vector $b \in \mathbb{B}^b$, e.g. the subset $\{1,5,7\}$ is mapped to vector $b$ where $b_i=1$ for $i \in \{1,5,7\}$ and $b_i=0$ otherwise.

\textbf{Polynomial representation}

\begin{align*}
	f(x_1,\cdots, x_n) = \sum_{S \subset \mathbf{V}}{c_S\prod_{j \in S}{x_j}},
\end{align*}

where $c_S \in \mathbb{R}$. The largest subset of $V$ with non zero coefficient is called the degree of $f$. The size of $f$ is defined as $size(f) = \sum_{c_S \neq 0}{|S|}$, i.e. the number of variable occurrences. 

The $i-th$ derivative of $f$ is defined as

\begin{align*}
	\Delta_i(x) &= f(x_1,\cdots, x_{i-1},1,x_{i+1},\cdots,x_n) - f(x_1,\cdots, x_{i-1},0,x_{i+1},\cdots,x_n) \\
	&= \sum_{S \subset \mathbf{V} \setminus \{i\}}{c_{S \cup \{i\}}\prod_{j \in S}{x_j}}.
\end{align*}

Its $i-th$ residual is defined as

\begin{align*}
	\Theta_i(x) &= f(x) - x_i\Delta_i(x) \\
	&= \sum_{S \subset \mathbf{V} \setminus \{i\}}{c_{S}\prod_{j \in S}{x_j}}.	
\end{align*}

We can also represent a PBF in its posiform representation. We identify each literal $x_i$ and $\overline{x_i}$ with a variable, and its coefficients are always non-negative.

\textbf{Posiform representation}

\begin{align*}
	\phi(x_1,\cdots,x_n) = \sum_{T \subset L}{a_T}\prod_{u \in T}{u},
\end{align*}

where $a_T \geq 0$.

\textbf{Example:} (Posiform and polynomial representation of same function)

\begin{align*}
	f(x) =& a_{\emptyset} + a_\ol{1} + a_\ol{2} + a_\ol{12} + \big(a_1 - a_\ol{1} + a_{1\ol{2}} - a_\ol{12}\big)x_1 \\
	&+ \big(a_{2} - a_\ol{2} + a_{\ol{1}2} - a_{\ol{12}}\big)x_2\\ 
	&+ \big(a_{12} - a_{\ol{1}2} - a_{1\ol{2}} + a_{\ol{12}}\big)x_1x_2 \\[1em]
	\phi(x) =& a_{\emptyset} + a_1x_1 + a_\ol{1}\ol{x_1} + a_2x_2 + a_\ol{2}\ol{x_2} \\ 
			&+ a_{12}x_1x_2 + a_{\ol{1}2}\ol{x_1}x_2 + a_{1\ol{2}}x_1\ol{x_2} + a_{\ol{12}}x_1\ol{x_2}
\end{align*}

The polynomial representation of a PBF is unique, but not its posiform.

\begin{align*}
	5x_1 + 4\ol{x_1}\ol{x_2}x_3 \Leftrightarrow 4\ol{x_2}x_3 + x_1 + 4x_1\ol{x_2}\ol{x_3} + 4x_1x_2.
\end{align*}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
$x_1$ & $x_2$ & $x_3$ & $f(x)$\\
\hline
0 & 0 & 0 & 0 \\
0 & 0 & 1 & 4 \\
0 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 \\
1 & 0 & 0 & 5 \\
1 & 0 & 1 & 5 \\
1 & 1 & 0 & 5 \\
1 & 1 & 1 & 5 \\
\hline
\end{tabular}
\end{center}

\begin{align*}
	f(x) &= 4\ol{x_1}\ol{x_2}x_3 + 5x_1\ol{x_2}\ol{x_3} + 5x_1\ol{x_2}x_3 + 5x_1x_2\ol{x_3} + 5x_1x_2x_3 \\
		 &= \mathbf{ 4\ol{x_1}\ol{x_2}x_3 } + x_1 + 4x_1\ol{x_2}\ol{x_3} + \mathbf{4x_1\ol{x_2}x_3} + 4x_1x_2\ol{x_3} + 4x_1x_2x_3 \\	
		 &= 4\ol{x_2}x_3 + x_1 + 4x_1\ol{x_2}\ol{x_3} + \mathbf{4x_1x_2\ol{x_3}} + \mathbf{4x_1x_2x_3} \\
		 &= 4\ol{x_2}x_3 + x_1 + 4x_1\ol{x_2}\ol{x_3} + 4x_1x_2 		 			 	
\end{align*}

Nonetheless, the posiform of $f$ in which $a_{\emptyset} = \min{f(x)}$ is unique and its called the \emph{min-term} posiform. Every PBF has a min-term posiform.

\begin{proposition}{Round-down}\label{prop:round-down}
Let $f:\mathbb{B}^n \rightarrow \mathbb{R}$ be a pseudo-boolean function and $g:\mathbb{U}^n\rightarrow \mathbb{R}$ its continuous counterpart constrained in the interval $\mathbb{U} = [0,1]$. For any $r \in \mathbb{U}^n$ there exists $q \in \mathbb{B}^n$ such that $f(q) \leq g(r)$.
\end{proposition}

Therefore, we can always find a binary lower bound for $g$. It can be shown that binary upper bounds can also be found. From these propositions we conclude that

\begin{align*}
	\argmin{f} = \argmin{g}
\end{align*}

Notice,  that lower and upper bounds can be found with respect to a initial vector $r \in \mathbb{U}^n$. Therefore, rounding procedures can be used to return local optima at most. Moreover, the round-down procedure may not produce a local optimal even if we consider the $1$-hamming distance.

\textbf{Example:}

\begin{proposition}{Quadratic reduction}\label{prop:quadratic-reduction}
	The optimization of a pseudo-boolean function of any order can always be restated as the optimization of a second order pseudo-boolean function.
\end{proposition} 

The proof is based in the following observation.

\begin{align*}
	xy = z &\Leftrightarrow xy - 2xz -2yz + 3z = 0 \\
	xy \neq z &\Leftrightarrow xy -2xz - 2yz + 3z > 0	
\end{align*}

Therefore, one can substitute variable pair $x_ix_j$ by including a penalty term of the form $M(xy - 2xz -2yz + 3z)$, where $M=1 + 2\sum_{S \subset V}{|c_S|}$. The problem to find the minimum number of variable pairs to be substituted is in $NP$-complete (one can reduce vertex cover to this problem).


\textbf{Basic algorithm}

Optimize the posiform representation of $f$ with unary coefficients is equivalent to find an assignment in which the maximum number of terms of $f$ are canceled. Such problem can be reduced from \texttt{MAX-SAT} and therefore is NP-complete.

However, there are some properties that help us to find optimal solutions. One of those is the persistency of a solution. We say that a $\mathbf(y) \subset \mathbb{B}^S$ is a partial assignment for $f$. For any assignment $\mathbf{x} \in \mathbb{B}^n$, we denote $\mathbf{x}[\leftarrow \mathbf{y}] $a relabeling by $\mathbf{y}$, that is

\[
	\mathbf{x}[\leftarrow \mathbf{y}]_i = \left\{ 
	\begin{array}{ll}
		\mathbf{x}_i & i \notin S\\
		\mathbf{y}_i & i \in S
	\end{array}\right.
\]

 We say that $\mathbf{y}$ is \emph{weak-persistent} if there exists optimal solutions in which $\mathbf{y}$ is part of it. In other words, there exists $\mathbf{x^\star}=\argmin f$ such that $\mathbf{x^\star}_i = \mathbf{y}_i \; \forall i \in S$. We say it is \emph{strong-persistent} if the partial assignment $\mathbf{y}$ is in every optimal solution.
 
 
There exists a case in which is easy to say that a partial assignment is weak-persistent. We say that a partial assignment is a \emph{contractor} if a relabeling by it eliminates all terms in which a variable from the assignment belongs to. A contractor is the best one can do w.r.t. to the variables in $S$, therefore, there exists optimal solutions in which such assignment is present. Determine if a partial assignment is a contractor can be done in linear time to $size(f)$.

\begin{proposition}{Necessary conditions for optimality}
	\[
		\forall i, \;	x_i = \left\{ \begin{array}{ll}
			1, & \Delta_i(x) < 0 \\
			0, & \Delta_i(x) > 0
		\end{array}\right.
	\]
\end{proposition}

From the necessary conditions there exists a naive algorithm that can be implemented. We start from a initial labeling, and the idea is to eliminate one variable at each iteration. At iteration $1$, the variable $x_1$ is eliminated. We replace $x_1$ by an boolean expression involving all other variables that evaluates to $1$ if and only if $\Delta_i(x)<0$ and evaluates to $0$ otherwise. Such algorithm is not used in practice because it has exponential complexity.

\textbf{Relation with stable sets}

A stable set of a graph $G$ is defined as its maximum independent set of vertices. For a given weight function $w$ for its vertices, the stability number of $G$ is defined as

\begin{align*}
	\alpha_w(G) = \max_{I \in V}\sum_{v \in I} w(v),\, I \text{an independent set}.
\end{align*} 

\begin{theorem}{}
For any posiform $\phi$, one can construct a graph $G$ such that
\begin{align*}
\max_{x \in \mathbb{B}^n}{\phi(x)} = a_{\emptyset} + \alpha_{a}(G).
\end{align*}
\end{theorem}

In particular, the vertices of $G$ are mapped to terms of $\phi$ and its weight mapped to the terms' coefficient. Moreover,

\begin{theorem}{}
Given a graph $G = (V, E)$ and nonnegative weights $a_i \geq 0$
associated to the vertices $i \in V$ , there exists a posiform $\phi_G$  in $n' < |V| $ variables, consisting of $|V|$ terms, and such that

\begin{align*}
	\alpha_a(G) = \max_{x \in \mathbb{B}^{n'} } \phi_G(x)
\end{align*}
\end{theorem}

The theorems above says not only that posiform maximization is as difficult to solve as maximum independent sat, but points out that they are somehow equivalent. This connection allow us to extend some results regarding approximation algorithms to maximum independent set to posiform maximiztion, in particular, its likely that a polynomial time constant factor approximation algorithm for it doesn' exist unless $P=NP$. Nonetheless, for some class of graphs there exists approximation algorithms for any factor $c<1$ (a PTAS scheme) which raises questions as: for which class of graphs posiforms are mapped to? Depending on the answer, the minimization of a posiform might be feasbile, or at least we can get a good approximation.

\textbf{Open question:} For the DGCI flow: To which class of graphs are mapped our posiforms? 

\textbf{Obs:} In the DGCI flow, we invert the solution at the end. I ran the following experiment. I execute two different flows. 
\begin{itemize}
	\item{*}{F1: Minimize p and invert labels.}
	\item{*}{F2: Minimize -p and keep labels.}	
\end{itemize}

The results are visually similar, though not equivalent. I've print the energy values at each iteration and they differ by a seemly bounded value. Such property is quite remarkable, because we don't expect that inverting the minimization problem solution we would get the solution of the maximization problem.

\textbf{Obs:} The minimum vertex cover is equal to the complement of the maximum independent set. As we have seen, there exists a graph $G_{\phi}$ such that the minimization of $\phi$ equals the stability number of $G_{\phi}$. If we can prove that the minimum vertex cover of $\phi$ equals maximizing $\phi$, then we are done.




Maximize or minimize a pbf in polynomial form are equivalent problems. Minimize a polynomial $p$ equals to maximize $-p$. With respect to posiforms, coeficients are always positive and such argument cannot be used.







\textbf{Paper colllection}
\begin{enumerate}
	\item{Quadratic reformulation of nonlinear binary optimization problems}
	\begin{itemize}
		\item{A Pseudo-Boolean function (PBF) $f:\{0,1\}\rightarrow \mathbb{R}$.}
		\item{It can be regarded as a set function; a PBF of any degree can be reduced to a quadratic PBF by adding enough variables.}
		\item{An strategy for solving a PBF is to transform it in a linear programming. Monomials are replaced by single variables and these variables are forced to obey some linearization constraints. }
	\end{itemize}
	\item{Nonconvex mixed-integer nonlinear programming: a survey}
	\begin{itemize}
		\item{Pointed by item 1 as a reference for strategies involving linear programming formulations.}
		\item{Convex MINLP are more tractable than nonconvex MINLP.}
		\item{Strategy 1: Linearization constraints. For each quadratic monomial create a new varible and three additional constraints;}
		\item{Strategy 2: Similar to above, but uses O(n) constraints instead of O($n^2$);}
		\item{Strategy 3: Convex relaxation by adding or subtracting terms of the form $x_i^2-x_i$.}
		\item{Strategy 4: Roof Dual (QPBO) }
		\item{Strategy 5: Semidefinite relaxation}
		\item{MINLP solvers: BARON, alpha-BB, Lindo-Global, Couenne.}
	\end{itemize}
	\item{Convex programming}
	\begin{itemize}
		\item{The item 2 made me review some definition from Boyd's book.}
		\item{Least-Squares: Analytical solution; necessary condition + newton root finding; gradient descent or similar.}
	\end{itemize}
	\item{ A tight bound for the boolean quadratic optimization
problem and its use in a branch and bound algorithm.}
	\begin{itemize}
		\item{Description of strategy 3.}
	\end{itemize}
	\item{A reformulation-linearization technique for solving discrete and continuous nonconvex problems. 2013}
	\item{Improving the performance of standard solvers for quadratic 0-1 programs by a tight convex reformulation: The QCR method. 2009}
	\item{On quadratization of pseudo-Boolean functions.}
	\item{An hypergraph-based reduction for higher-order Markov
random fields.}
	\item{A comparative study of modern inference techniques for discrete energy minimization problems.}
	\item{Pseudo-Boolean optimization}
	\begin{itemize}
		\item{The costumes of PBF: Set function; posiform (literals and positive coefficients); multilinear polynomial.}
		\item{The derivative, its residual residual and its size?}
		\item{The min term representation. The unique posiform in which $f(0) = \min f$.}
		\item{Let a PBF in its multilinear polynomial form. Moreover, relax the binary domain to accomodate the interval $[0,1]$. For any value of $r \in [0,1]^n$ there are binary vectors $x,y$ that lower and upper bounds $f(r)$. }
		\item{Local search.}
		\item{If the PBF is convex, we can apply standard continuous methods and use the roundup roundown procedures. Convex recognition [41,42,124]; Convexification [90,117].}
		\item{Quadratic Optimization reduction: }
	\end{itemize}
	\item{Convex Analysis and Optimization with Submodular functions: a tutorial}
	\begin{itemize}
	\item{Lovasz extension: A function is submodular is and only if its Lovazs extension is convex.}
	\item{Relation with total variation.}
	\end{itemize}
\end{enumerate}